# ğŸš€ EKS Autoscaling com KEDA e Karpenter

<p align="center">
  <a href="img/aws_kedakarpenter_arch_small.gif">
    <img src="img/aws_kedakarpenter_arch_static.png" alt="Arquitetura EKS KEDA Karpenter" width="800" />
  </a>
</p>

> ğŸ¬ **[Veja a animaÃ§Ã£o completa da arquitetura](img/aws_kedakarpenter_arch_small.gif)** 

<p align="center">
  <img src="https://img.shields.io/badge/AWS-EKS_1.31-FF9900?style=for-the-badge&logo=amazon-aws&logoColor=white" />
  <img src="https://img.shields.io/badge/Karpenter-1.0.1-326CE5?style=for-the-badge&logo=kubernetes&logoColor=white" />
  <img src="https://img.shields.io/badge/KEDA-2.15.1-2496ED?style=for-the-badge&logo=kubernetes&logoColor=white" />
  <img src="https://img.shields.io/badge/Kubernetes-1.31-326CE5?style=for-the-badge&logo=kubernetes&logoColor=white" />
  <img src="https://img.shields.io/badge/Prometheus-Monitoring-E6522C?style=for-the-badge&logo=prometheus&logoColor=white" />
  <img src="https://img.shields.io/badge/Grafana-Dashboards-F46800?style=for-the-badge&logo=grafana&logoColor=white" />
</p>

---

## ğŸ“‹ Sobre o Projeto

Este lab demonstra **autoscaling avanÃ§ado no Kubernetes** usando:
- **AWS EKS** 1.31
- **Karpenter** 1.0.1 (Node Autoscaling)
- **KEDA** 2.15.1 (Pod Autoscaling)

> ğŸ¬ **[Veja a animaÃ§Ã£o completa da arquitetura](img/aws_kedakarpenter_arch_small.gif)** 


### ğŸ¯ CenÃ¡rios Validados

#### 1. ğŸ“Š **Processamento de Filas SQS**
- âœ… Escala automÃ¡tica de **1 â†’ 50+ pods** baseado em mensagens SQS
- âœ… KEDA monitora fila FIFO em tempo real
- âœ… Karpenter provisiona novos nÃ³s em **60-90 segundos**
- âœ… Processamento de pagamentos com persistÃªncia no DynamoDB

#### 2. ğŸ–¥ï¸ **Node Scaling com Karpenter**
- âœ… Provisionamento automÃ¡tico de nodes EC2
- âœ… Scale-down inteligente apÃ³s 30s sem carga
- âœ… Suporte a mÃºltiplos instance types (m5.large, m5.xlarge, m5.2xlarge)

---
## âœ¨ **VERSÃƒO CORRIGIDA E FUNCIONAL**

> **Esta Ã© a versÃ£o atualizada do lab Eks-Keda_karpenter**, completamente refatorada e testada com as APIs mais recentes do Karpenter e KEDA.

---

## ğŸ¯ Features atualizadas

| # | SoluÃ§Ã£o Implementada |
|---|-------------------|---------------------|
| 1 | âœ… Karpenter aualizado para **v1.0.1** com APIs `v1` estÃ¡veis |
| 2 | âœ… Migrado para **NodePool** e **EC2NodeClass** |
| 3 | âœ… Atualizado para **KEDA v2.15.1** com API estÃ¡vel |
| 4 | âœ… ConfiguraÃ§Ã£o automÃ¡tica de tags em subnets e SGs |
| 5 | âœ… Trust policies corrigidas e testadas |
| 6 | âœ… Pods com `requests: 500m CPU` para forÃ§ar scaling |
| 7 | âœ… ValidaÃ§Ã£o completa em cada etapa |
| 8 | âœ… DependÃªncias verificadas automaticamente |

---

## ğŸ”§ PrÃ©-requisitos

### ğŸ“¦ Ferramentas NecessÃ¡rias

```bash
# Verificar instalaÃ§Ã£o
aws --version      # AWS CLI 2.x+
kubectl version    # kubectl 1.28+
eksctl version     # eksctl 0.150+
helm version       # Helm 3.x+
python3 --version  # Python 3.8+
```

### â˜ï¸ Requisitos AWS

- Conta AWS ativa
- Credenciais configuradas: `aws configure`
- PermissÃµes IAM para EKS, EC2, VPC, SQS, DynamoDB, IAM, CloudFormation

---

## ğŸš€ InstalaÃ§Ã£o RÃ¡pida (25 minutos)

### 1ï¸âƒ£ Clone o RepositÃ³rio

```bash
# Escolha o diretÃ³rio de sua preferÃªncia
cd ~

git clone https://github.com/jlui70/lab-eks-keda-karpenter.git
cd lab-eks-keda-karpenter
```

### 2ï¸âƒ£ Configure VariÃ¡veis (Opcional)

```bash
nano deployment/environmentVariables.sh
```

**Valores padrÃ£o funcionam perfeitamente:**
- Cluster: `eks-demo-scale-v2`
- RegiÃ£o: `us-east-1`
- Karpenter: `v1.0.1`
- KEDA: `v2.15.1`

### 3ï¸âƒ£ (Recomendado) Execute PrÃ©-VerificaÃ§Ã£o

âš ï¸ **IMPORTANTE:** Se vocÃª jÃ¡ executou este lab anteriormente e fez cleanup, execute a prÃ©-verificaÃ§Ã£o para garantir que nÃ£o hÃ¡ recursos Ã³rfÃ£os:

```bash
./scripts/pre-install-check.sh
```

Este script vai:
- âœ… Verificar se o cluster jÃ¡ existe
- âœ… Limpar CloudFormation stacks Ã³rfÃ£s (`DELETE_FAILED`)
- âœ… Remover VPCs Ã³rfÃ£s
- âœ… Deletar IAM Roles Ã³rfÃ£s

ğŸ’¡ **Quando executar:**
- Sempre que tentar reinstalar apÃ³s um cleanup
- Se encontrar erro de "Stack already exists"
- Se o cleanup anterior falhou

### 4ï¸âƒ£ Execute Deployment Completo

```bash
chmod +x deployment/_main.sh
./deployment/_main.sh
```

â±ï¸ **Tempo total: ~25 minutos**

```
Etapa 1/4: Cluster EKS .......... 15-20 min
Etapa 2/4: Karpenter ............ 3-5 min
Etapa 3/4: KEDA ................. 2-3 min
Etapa 4/4: AWS Services ......... 1 min
```

---
## ğŸ§ª Executando os Testes

### âš™ï¸ PreparaÃ§Ã£o do ambiente para visualizaÃ§Ã£o do teste

#### ğŸ› ï¸ OpÃ§Ã£o 1: Usando K9s (Recomendado)

**K9s** Ã© uma interface CLI interativa para Kubernetes que facilita muito a visualizaÃ§Ã£o e navegaÃ§Ã£o dos recursos.

**InstalaÃ§Ã£o do K9s:**

```bash
# Linux
curl -sS https://webinstall.dev/k9s | bash

# macOS (Homebrew)
brew install derailed/k9s/k9s

# Verificar instalaÃ§Ã£o
k9s version
```

Abra **4 terminais** side-by-side:

**Terminal 1 - Pods (namespace keda-test):**
```bash
# OpÃ§Ã£o A: VisualizaÃ§Ã£o via kubectl
watch kubectl get pods -n keda-test

# OpÃ§Ã£o B: VisualizaÃ§Ã£o via k9s (abre direto nos pods do namespace)
k9s -n keda-test
# Dentro do k9s: digite :pods (Enter) para ver Pods keda-test
```

**Terminal 2 - HPA / ScaledObject:**
```bash
# OpÃ§Ã£o A: VisualizaÃ§Ã£o via kubectl
watch kubectl get hpa -n keda-test

# OpÃ§Ã£o B: VisualizaÃ§Ã£o via k9s (dentro do k9s digite :hpa)
k9s -n keda-test
# Dentro do k9s: digite :hpa (Enter) para ver HorizontalPodAutoscaler
```

**Terminal 3 - Nodes (cluster):**
```bash
# OpÃ§Ã£o A: VisualizaÃ§Ã£o via kubectl
watch kubectl get nodes

# OpÃ§Ã£o B: VisualizaÃ§Ã£o via k9s (dentro do k9s digite :nodes)
k9s -n nodes
# Dentro do k9s: digite :nodes (Enter) para ver os nodes do cluster
```

**Terminal 4 - Fila SQS (opcional - somente via AWS CLI):**
```bash
# Monitorar quantidade de mensagens na fila em tempo real
watch -n 5 'aws sqs get-queue-attributes \
  --queue-url https://sqs.us-east-1.amazonaws.com/794038226274/keda-demo-queue.fifo \
  --attribute-names ApproximateNumberOfMessages \
  --query "Attributes.ApproximateNumberOfMessages" \
  --output text'
```

ğŸ’¡ **Dica K9s:** 
- Use `k9s -n <namespace>` para abrir direto no namespace desejado
- Dentro do k9s, navegue com `:pods`, `:hpa`, `:nodes`, `:scaledobject`
- Pressione `Enter` em um recurso para ver detalhes e logs
- Pressione `Ctrl+C` para voltar ou sair

---

### ğŸ“Š Teste SQS Scaling

```bash
cd tests
chmod +x run-load-test.sh
./run-load-test.sh
```

**O script vai perguntar quantas mensagens enviar:**

```
OpÃ§Ã£o 1: Digite um nÃºmero (ex: 500)
OpÃ§Ã£o 2: Digite 'continuous' para modo contÃ­nuo
```

ğŸ’¡ **SugestÃ£o para testes rÃ¡pidos:** Envie **500 mensagens** para observar o scaling em aÃ§Ã£o. Nos testes de laboratÃ³rio, esse volume demonstrou claramente o comportamento do sistema:
- âš¡ **~3 minutos** para escalar atÃ© **50 pods**
- ğŸš€ **10 nodes** provisionados automaticamente pelo Karpenter
- ğŸ“Š Ideal para validar KEDA + Karpenter trabalhando juntos

### ğŸ¯ O que esperar:

1. âœ… **0-30s**: KEDA detecta mensagens e comeÃ§a a escalar pods
2. âœ… **30-60s**: Pods ficam `Pending` (aguardando nodes)
3. âœ… **60-90s**: Karpenter provisiona novos nodes EC2
4. âœ… **90-120s**: Pods sÃ£o agendados e comeÃ§am a processar
5. âœ… **ApÃ³s fila esvaziar + 30s**: Scale-down automÃ¡tico

---

### âš¡ Scale-Down RÃ¡pido (Para LaboratÃ³rio)

âš ï¸ **IMPORTANTE:** Em produÃ§Ã£o, o KEDA e Karpenter fazem o scale-down automaticamente conforme as configuraÃ§Ãµes (cooldown periods, thresholds, etc.). No entanto, para **economizar custos durante testes em laboratÃ³rio**, vocÃª pode forÃ§ar um scale-down imediato apÃ³s validar o comportamento do sistema.

**Comandos para Scale-Down Imediato:**

```bash
# 1ï¸âƒ£ Limpar todas as mensagens da fila SQS
aws sqs purge-queue \
  --queue-url $(aws sqs get-queue-url --queue-name keda-demo-queue.fifo --query 'QueueUrl' --output text)

# 2ï¸âƒ£ Deletar o ScaledObject (KEDA para de escalar)
kubectl delete scaledobject sqs-scaledobject -n keda-test

# 3ï¸âƒ£ Escalar manualmente o deployment para 1 rÃ©plica
kubectl scale deployment sqs-app -n keda-test --replicas=1

# 4ï¸âƒ£ Verificar pods sendo removidos
kubectl get pods -n keda-test

# 5ï¸âƒ£ Aguardar ~60s e verificar nodes sendo removidos pelo Karpenter
watch kubectl get nodes
```

**Recriar o ScaledObject apÃ³s os testes (opcional):**

```bash
# Se quiser fazer novos testes, recrie o ScaledObject
kubectl apply -f deployment/app/scaledobject.yaml
```

ğŸ’¡ **ExplicaÃ§Ã£o:**
- **ProduÃ§Ã£o:** O KEDA aguarda o `cooldownPeriod` (300s padrÃ£o) apÃ³s a fila esvaziar para fazer scale-down gradual
- **LaboratÃ³rio:** Limpamos a fila e removemos o ScaledObject para scale-down instantÃ¢neo e economia de custos
- **Karpenter:** Remove nodes automaticamente apÃ³s ~60s quando nÃ£o hÃ¡ pods agendados neles

---

## ğŸ“Š Monitoramento com Prometheus + Grafana

### ğŸ¨ Dashboards Customizados

O projeto inclui stack completa de monitoramento com 2 dashboards profissionais:

#### **1. SQS Payments Dashboard**
- ğŸ“¨ Mensagens processadas em tempo real
- ğŸš€ NÃºmero de pods ativos (KEDA scaling)
- ğŸ’» UtilizaÃ§Ã£o de CPU/MemÃ³ria
- âš¡ Taxa de processamento (msgs/s)
- ğŸ“Š HistÃ³rico de scaling

### ğŸ“ Acessar Grafana

# Port-Forward (local)
kubectl port-forward svc/monitoring-grafana 3000:80 -n monitoring
```

Acesse: **http://localhost:3000**

**Credenciais padrÃ£o:**
```
UsuÃ¡rio: admin
Senha: admin123
```

### ğŸ” Verificar MÃ©tricas no Prometheus

```bash
# Port-forward Prometheus
kubectl port-forward svc/monitoring-kube-prometheus-prometheus 9090:9090 -n monitoring
```

Acesse: **http://localhost:9090**

**Queries Ãºteis:**
```promql
# Mensagens na fila SQS
aws_sqs_approximate_number_of_messages

# Pods ativos KEDA
kube_deployment_status_replicas{namespace="keda-test"}

# Nodes Karpenter
karpenter_nodes_total
```

ğŸ“š **DocumentaÃ§Ã£o completa**: [monitoring/README.md](monitoring/README.md)

---

## ğŸ§¹ Limpeza de Recursos

âš ï¸ **IMPORTANTE:** Execute apÃ³s os testes para evitar custos!

```bash
cd scripts
chmod +x cleanup.sh
./cleanup.sh
```

**Digite `DELETE` para confirmar.**

O script remove:
- âœ… Cluster EKS completo
- âœ… Todos os nodes EC2
- âœ… VPC, subnets, NAT gateways
- âœ… SQS queue
- âœ… DynamoDB table
- âœ… IAM roles e policies
- âœ… CloudFormation stacks

â±ï¸ **Tempo: ~10-15 minutos**

### ğŸ”§ Troubleshooting do Cleanup

**Problema: Cleanup termina mas stacks ficam em DELETE_FAILED**

Isso pode acontecer se houver dependÃªncias entre recursos. O cleanup agora forÃ§a a deleÃ§Ã£o, mas vocÃª pode precisar verificar:

```bash
# 1. Verificar stacks Ã³rfÃ£s
aws cloudformation list-stacks \
  --stack-status-filter DELETE_FAILED CREATE_FAILED \
  --region us-east-1 \
  --query 'StackSummaries[?contains(StackName, `eks-demo-scale-v2`)].{Name:StackName,Status:StackStatus}'

# 2. Se encontrar stacks Ã³rfÃ£as, execute o script de prÃ©-verificaÃ§Ã£o
./scripts/pre-install-check.sh
```

**Recursos Ã³rfÃ£os comuns (NÃƒO geram custo):**
- âœ… CloudFormation stacks em DELETE_FAILED (sem custo)
- âœ… VPC sem recursos ativos (sem custo)
- âœ… Security Groups Ã³rfÃ£os (sem custo)
- âœ… IAM Roles/Policies (sem custo)

**Recursos que GERAM custo (sÃ£o sempre deletados primeiro):**
- âŒ EC2 Instances
- âŒ NAT Gateways
- âŒ EKS Control Plane
- âŒ Load Balancers

ğŸ’¡ **Dica:** Se quiser verificar manualmente se hÃ¡ custos, acesse o [AWS Cost Explorer](https://console.aws.amazon.com/cost-management/home?#/home)

---

## ğŸ’° Custos Estimados

| Recurso | Custo/hora | Custo Lab (3h) |
|---------|-----------|----------------|
| EKS Control Plane | $0.10 | $0.30 |
| NAT Gateways (3x) | $0.135 | $0.40 |
| EC2 Nodes (2-5x m5.large) | ~$0.50 | ~$1.50 |
| SQS + DynamoDB | < $0.01 | < $0.01 |
| **TOTAL** | **~$0.75/h** | **~$2.00** |

ğŸ’¡ **Dica:** Execute `cleanup.sh` imediatamente apÃ³s os testes!

---

## ğŸ”— Links Ãšteis

- [Karpenter v1 Migration Guide](https://karpenter.sh/docs/upgrading/v1-migration/)
- [KEDA v2 ScaledObject Spec](https://keda.sh/docs/latest/concepts/scaling-deployments/)
- [AWS EKS Best Practices](https://aws.github.io/aws-eks-best-practices/)

---

## ğŸ™ CrÃ©ditos e Agradecimentos

### ğŸ“š VersÃ£o Original

**Projeto Base:** [aws-samples/amazon-eks-scaling-with-keda-and-karpenter](https://github.com/aws-samples/amazon-eks-scaling-with-keda-and-karpenter)

### ğŸŒŸ Tecnologias Utilizadas

Agradecimentos especiais Ã s comunidades open-source:

- **AWS EKS** - Managed Kubernetes service
- **Karpenter** - Just-in-time node provisioning
- **KEDA** - Kubernetes Event-driven Autoscaling
- **Prometheus** - Cloud-native monitoring
- **Grafana** - Metrics visualization
- **Helm** - Kubernetes package manager
- **Python** - Application development
- **eksctl** - EKS cluster management

### ğŸ’™ Comunidade

Este projeto foi criado para ajudar a comunidade brasileira de DevOps, SRE e Cloud Engineering a implementar autoscaling avanÃ§ado de forma prÃ¡tica e automatizada.

---

## ğŸ“ Contato e Suporte

### ğŸŒ Conecte-se Comigo

- ğŸ“¹ **YouTube:** [DevOps Project](https://www.youtube.com/@devops-project)
- ğŸ’¼ **PortfÃ³lio:** [devopsproject.com.br](https://devopsproject.com.br/)
- ğŸ’» **GitHub:** [@jlui70](https://github.com/jlui70)

### ğŸ’¬ Precisa de Ajuda?

- ğŸ› Abra uma [issue no repositÃ³rio](https://github.com/jlui70/lab-eks-keda-karpenter/issues)
- ğŸ’¡ Participe das discussÃµes no YouTube
- ğŸ“§ Entre em contato via portfÃ³lio

### ğŸŒŸ Gostou do Projeto?

Se este projeto foi Ãºtil para vocÃª:

- â­ DÃª uma **estrela** no [repositÃ³rio](https://github.com/jlui70/lab-eks-keda-karpenter)
- ğŸ”„ **Compartilhe** com a comunidade
- ğŸ“¹ **Inscreva-se** no canal do YouTube
- ğŸ¤ **Contribua** com melhorias

---

## ğŸ“„ LicenÃ§a

MIT License - Este projeto estÃ¡ licenciado sob a licenÃ§a MIT.

---

<p align="center">
  <strong>Desenvolvido com â¤ï¸ para a comunidade brasileira de DevOps, SRE e Cloud Engineering</strong>
</p>

<p align="center">
  <sub>EKS Autoscaling com KEDA e Karpenter â€¢ 2025-2026 â€¢ Todos os direitos reservados</sub>
</p>
